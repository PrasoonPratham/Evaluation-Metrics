# Evaluation Metrics

We focus on evaluation metrics, because they do not have extensive dependencies on the intricacies of Artificial Intelligence, but only require the very basics of mathematical analysis and statistics to understand how they tick. They are afterall, only a formal way of measuring how well an answer corresponds to a question. With a focus on Evaluation Metrics, we become the gatekeepers of all the research activity that is conducted by the Research community in Artificial Intelligence.

Unfortunately, not many researchers still have deep intuitions behind many of the evaluation metrics we regularly use. We still treat them as just numbers that we have to optimize.

In this repository, we will focus on building a set of resources which will help anyone build a fundamental intuition around many of these evaluation metrics. We will think about these evaluation metrics from the first principles, and ask questions like, why does this particular evaluation metric even need to question ? What is its defining factor ? For what classes of problems is this metric used ? What are the common things we should be careful about when we are using this evaluation metric ? So on and so forth. We would like to release these resources as a set of open source resources for the benefit of the community under an Open Source license.



